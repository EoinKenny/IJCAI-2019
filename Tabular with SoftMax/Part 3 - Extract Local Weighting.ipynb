{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Extract Local Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "\n",
    "# For Deep Learning Explanations\n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "import deeplift\n",
    "from deeplift.conversion import kerasapi_conversion as kc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"X_train.npy\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_test = np.load(\"y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('NN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Generate Gradient & Significance LIME Matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_predict(i):\n",
    "    \"\"\"\n",
    "    LIME doesn't support the format Keras uses, so we need a small helper function\n",
    "    \n",
    "    This just predicts a probability and gives the alternative also.\n",
    "    \"\"\"\n",
    "    \n",
    "    global model\n",
    "    \n",
    "    idx = model.predict_classes(i)\n",
    "    probs = model.predict_proba(i)\n",
    "    idx = np.array([idx]).T\n",
    "    \n",
    "    probability_yes = probs[np.arange(probs.shape[0])[:, None], idx]\n",
    "    \n",
    "    x = np.zeros((probability_yes.shape[0], 1))\n",
    "    probability_no = (x + 1) - probability_yes\n",
    "    final = np.append(probability_no, probability_yes, axis=1)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_features_ohe(df):\n",
    "    \"\"\"\n",
    "    Locate which features are categorical ohe, they should just have 2 values 0 and 1\n",
    "    \n",
    "    return: a list of integers referring to the ohe indexes\n",
    "    \"\"\"\n",
    "    \n",
    "    categorical_features = list()\n",
    "    for i in range(len(df.columns)):\n",
    "        if df[df.columns[i]].value_counts().shape == (2,):\n",
    "            categorical_features.append(i)\n",
    "    return categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = which_features_ohe(df)\n",
    "feature_names = df.columns\n",
    "num_features = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, \n",
    "                                                   feature_names=feature_names, \n",
    "                                                   categorical_features=categorical_features,\n",
    "                                                   class_names=[\"0\", \"1\", \"2\"],\n",
    "                                                   verbose=False, \n",
    "                                                   discretize_continuous=False,\n",
    "                                                   mode='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % done...\n",
      "1.644736842105263 % done...\n",
      "3.289473684210526 % done...\n",
      "4.934210526315789 % done...\n",
      "6.578947368421052 % done...\n",
      "8.223684210526317 % done...\n",
      "9.868421052631579 % done...\n",
      "11.513157894736842 % done...\n",
      "13.157894736842104 % done...\n",
      "14.802631578947366 % done...\n",
      "16.447368421052634 % done...\n",
      "18.092105263157894 % done...\n",
      "19.736842105263158 % done...\n",
      "21.38157894736842 % done...\n",
      "23.026315789473685 % done...\n",
      "24.671052631578945 % done...\n",
      "26.31578947368421 % done...\n",
      "27.960526315789476 % done...\n",
      "29.605263157894733 % done...\n",
      "31.25 % done...\n",
      "32.89473684210527 % done...\n",
      "34.53947368421053 % done...\n",
      "36.18421052631579 % done...\n",
      "37.82894736842105 % done...\n",
      "39.473684210526315 % done...\n",
      "41.118421052631575 % done...\n",
      "42.76315789473684 % done...\n",
      "44.40789473684211 % done...\n",
      "46.05263157894737 % done...\n",
      "47.69736842105263 % done...\n",
      "49.34210526315789 % done...\n",
      "50.98684210526315 % done...\n",
      "52.63157894736842 % done...\n",
      "54.276315789473685 % done...\n",
      "55.92105263157895 % done...\n",
      "57.56578947368421 % done...\n",
      "59.210526315789465 % done...\n",
      "60.85526315789473 % done...\n",
      "62.5 % done...\n",
      "64.14473684210526 % done...\n",
      "65.78947368421053 % done...\n",
      "67.43421052631578 % done...\n",
      "69.07894736842105 % done...\n",
      "70.72368421052632 % done...\n",
      "72.36842105263158 % done...\n",
      "74.01315789473685 % done...\n",
      "75.6578947368421 % done...\n",
      "77.30263157894737 % done...\n",
      "78.94736842105263 % done...\n",
      "80.5921052631579 % done...\n",
      "82.23684210526315 % done...\n",
      "83.88157894736842 % done...\n",
      "85.52631578947368 % done...\n",
      "87.17105263157895 % done...\n",
      "88.81578947368422 % done...\n",
      "90.46052631578947 % done...\n",
      "92.10526315789474 % done...\n",
      "93.75 % done...\n",
      "95.39473684210526 % done...\n",
      "97.03947368421053 % done...\n",
      "98.68421052631578 % done...\n"
     ]
    }
   ],
   "source": [
    "X_train_grad_LIME = list()\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print((i / len(X_train)) * 100, \"% done...\")\n",
    "    \n",
    "    qc = X_train[i]\n",
    "    exp = explainer.explain_instance(qc, flatten_predict, num_features=num_features)\n",
    "    \n",
    "    # Get real coefficients\n",
    "    coefs = exp.as_map()[1]\n",
    "    coefs.sort()\n",
    "    coefs = [x[1] for x in coefs]\n",
    "\n",
    "    X_train_grad_LIME.append(np.append(coefs, exp.intercept[1]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % done...\n",
      "14.801657785671996 % done...\n",
      "29.60331557134399 % done...\n",
      "44.40497335701599 % done...\n",
      "59.20663114268798 % done...\n",
      "74.00828892835997 % done...\n",
      "88.80994671403198 % done...\n"
     ]
    }
   ],
   "source": [
    "X_test_grad_LIME = list()\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print((i / len(X_test)) * 100, \"% done...\")\n",
    "    \n",
    "    qc = X_test[i]\n",
    "    exp = explainer.explain_instance(qc, flatten_predict, num_features=num_features)\n",
    "    \n",
    "    # Get real coefficients\n",
    "    coefs = exp.as_map()[1]\n",
    "    coefs.sort()\n",
    "    coefs = [x[1] for x in coefs]\n",
    "\n",
    "    X_test_grad_LIME.append(np.append(coefs, exp.intercept[1]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_grad_LIME = np.array(X_train_grad_LIME)\n",
    "X_test_grad_LIME = np.array(X_test_grad_LIME)\n",
    "np.save(\"X_train_grad_LIME\", X_train_grad_LIME)\n",
    "np.save(\"X_test_grad_LIME\", X_test_grad_LIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients and Layerwise Relevance Propagation\n",
    "https://github.com/marcoancona/DeepExplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "nn_preds_train = model.predict_classes(X_train)\n",
    "nn_preds_test = model.predict_classes(X_test)\n",
    "oh_y_train = to_categorical(nn_preds_train)\n",
    "oh_y_test = to_categorical(nn_preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepExplain: running \"intgrad\" explanation method (3)\n",
      "Model with multiple inputs:  False\n",
      "DeepExplain: running \"elrp\" explanation method (4)\n",
      "Model with multiple inputs:  False\n",
      "DeepExplain: running \"intgrad\" explanation method (3)\n",
      "Model with multiple inputs:  False\n",
      "DeepExplain: running \"elrp\" explanation method (4)\n",
      "Model with multiple inputs:  False\n"
     ]
    }
   ],
   "source": [
    "with DeepExplain(session=K.get_session()) as de:  # <-- init DeepExplain context\n",
    "    input_tensor = model.layers[0].input\n",
    "    fModel = Model(inputs=input_tensor, outputs = model.layers[-2].output)\n",
    "    target_tensor = fModel(input_tensor)\n",
    "    \n",
    "    xs = X_train\n",
    "    \n",
    "    X_train_intgrad = de.explain('intgrad', target_tensor * oh_y_train, input_tensor, xs)\n",
    "    X_train_lrp = de.explain('elrp', target_tensor * oh_y_train, input_tensor, xs)\n",
    "    \n",
    "    \n",
    "with DeepExplain(session=K.get_session()) as de:  # <-- init DeepExplain context\n",
    "    input_tensor = model.layers[0].input\n",
    "    fModel = Model(inputs=input_tensor, outputs = model.layers[-2].output)\n",
    "    target_tensor = fModel(input_tensor)\n",
    "    \n",
    "    xs = X_test\n",
    "    \n",
    "    X_test_intgrad = de.explain('intgrad', target_tensor * oh_y_test, input_tensor, xs)\n",
    "    X_test_lrp = de.explain('elrp', target_tensor * oh_y_test, input_tensor, xs)\n",
    "    \n",
    "# Save the integrated gradients and LRP \n",
    "np.save(\"X_train_intgrad\", X_train_intgrad)\n",
    "np.save(\"X_test_intgrad\", X_test_intgrad)\n",
    "np.save(\"X_train_lrp\", X_train_lrp)\n",
    "np.save(\"X_test_lrp\", X_test_lrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLIFT Contributions:\n",
    "https://github.com/kundajelab/deeplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonlinear_mxts_mode is set to: DeepLIFT_GenomicsDefault\n",
      "For layer 1 the preceding linear layer is 0 of type Dense;\n",
      "In accordance with nonlinear_mxts_modeDeepLIFT_GenomicsDefault we are setting the NonlinearMxtsMode to RevealCancel\n",
      "Heads-up: I assume softmax is the output layer, not an intermediate one; if it's an intermediate layer, please let me know and I will prioritise that use-case\n",
      "For layer 3 the preceding linear layer is 2 of type Dense;\n",
      "In accordance with nonlinear_mxts_modeDeepLIFT_GenomicsDefault we are setting the NonlinearMxtsMode to RevealCancel\n",
      "No reference provided - using zeros\n",
      "Done 0\n",
      "Done 1000\n",
      "Done 2000\n",
      "Done 3000\n",
      "Done 4000\n",
      "Done 5000\n",
      "Done 6000\n",
      "Done 7000\n",
      "Done 8000\n",
      "Done 9000\n",
      "Done 10000\n",
      "Done 11000\n",
      "Done 12000\n",
      "Done 13000\n",
      "Done 14000\n",
      "Done 15000\n",
      "Done 16000\n",
      "Done 17000\n",
      "Done 18000\n",
      "Done 19000\n",
      "Done 20000\n",
      "Done 21000\n",
      "Done 22000\n",
      "Done 23000\n",
      "Done 24000\n",
      "Done 25000\n",
      "Done 26000\n",
      "Done 27000\n",
      "Done 28000\n",
      "Done 29000\n",
      "Done 30000\n",
      "Done 31000\n",
      "Done 32000\n",
      "Done 33000\n",
      "Done 34000\n",
      "Done 35000\n",
      "Done 36000\n",
      "Done 37000\n",
      "Done 38000\n",
      "Done 39000\n",
      "Done 40000\n",
      "Done 41000\n",
      "Done 42000\n",
      "Done 43000\n",
      "Done 44000\n",
      "Done 45000\n",
      "Done 46000\n",
      "Done 47000\n",
      "Done 48000\n",
      "Done 49000\n",
      "Done 50000\n",
      "Done 51000\n",
      "Done 52000\n",
      "Done 53000\n",
      "Done 54000\n",
      "Done 55000\n",
      "Done 56000\n",
      "Done 57000\n",
      "Done 58000\n",
      "Done 59000\n",
      "Done 60000\n",
      "No reference provided - using zeros\n",
      "Done 0\n",
      "Done 1000\n",
      "Done 2000\n",
      "Done 3000\n",
      "Done 4000\n",
      "Done 5000\n",
      "Done 6000\n",
      "No reference provided - using zeros\n",
      "Done 0\n",
      "Done 1000\n",
      "Done 2000\n",
      "Done 3000\n",
      "Done 4000\n",
      "Done 5000\n",
      "Done 6000\n",
      "Done 7000\n",
      "Done 8000\n",
      "Done 9000\n",
      "Done 10000\n",
      "Done 11000\n",
      "Done 12000\n",
      "Done 13000\n",
      "Done 14000\n",
      "Done 15000\n",
      "Done 16000\n",
      "Done 17000\n",
      "Done 18000\n",
      "Done 19000\n",
      "Done 20000\n",
      "Done 21000\n",
      "Done 22000\n",
      "Done 23000\n",
      "Done 24000\n",
      "Done 25000\n",
      "Done 26000\n",
      "Done 27000\n",
      "Done 28000\n",
      "Done 29000\n",
      "Done 30000\n",
      "Done 31000\n",
      "Done 32000\n",
      "Done 33000\n",
      "Done 34000\n",
      "Done 35000\n",
      "Done 36000\n",
      "Done 37000\n",
      "Done 38000\n",
      "Done 39000\n",
      "Done 40000\n",
      "Done 41000\n",
      "Done 42000\n",
      "Done 43000\n",
      "Done 44000\n",
      "Done 45000\n",
      "Done 46000\n",
      "Done 47000\n",
      "Done 48000\n",
      "Done 49000\n",
      "Done 50000\n",
      "Done 51000\n",
      "Done 52000\n",
      "Done 53000\n",
      "Done 54000\n",
      "Done 55000\n",
      "Done 56000\n",
      "Done 57000\n",
      "Done 58000\n",
      "Done 59000\n",
      "Done 60000\n",
      "No reference provided - using zeros\n",
      "Done 0\n",
      "Done 1000\n",
      "Done 2000\n",
      "Done 3000\n",
      "Done 4000\n",
      "Done 5000\n",
      "Done 6000\n",
      "No reference provided - using zeros\n",
      "Done 0\n",
      "Done 1000\n",
      "Done 2000\n",
      "Done 3000\n",
      "Done 4000\n",
      "Done 5000\n",
      "Done 6000\n",
      "Done 7000\n",
      "Done 8000\n",
      "Done 9000\n",
      "Done 10000\n",
      "Done 11000\n",
      "Done 12000\n",
      "Done 13000\n",
      "Done 14000\n",
      "Done 15000\n",
      "Done 16000\n",
      "Done 17000\n",
      "Done 18000\n",
      "Done 19000\n",
      "Done 20000\n",
      "Done 21000\n",
      "Done 22000\n",
      "Done 23000\n",
      "Done 24000\n",
      "Done 25000\n",
      "Done 26000\n",
      "Done 27000\n",
      "Done 28000\n",
      "Done 29000\n",
      "Done 30000\n",
      "Done 31000\n",
      "Done 32000\n",
      "Done 33000\n",
      "Done 34000\n",
      "Done 35000\n",
      "Done 36000\n",
      "Done 37000\n",
      "Done 38000\n",
      "Done 39000\n",
      "Done 40000\n",
      "Done 41000\n",
      "Done 42000\n",
      "Done 43000\n",
      "Done 44000\n",
      "Done 45000\n",
      "Done 46000\n",
      "Done 47000\n",
      "Done 48000\n",
      "Done 49000\n",
      "Done 50000\n",
      "Done 51000\n",
      "Done 52000\n",
      "Done 53000\n",
      "Done 54000\n",
      "Done 55000\n",
      "Done 56000\n",
      "Done 57000\n",
      "Done 58000\n",
      "Done 59000\n",
      "Done 60000\n",
      "No reference provided - using zeros\n",
      "Done 0\n",
      "Done 1000\n",
      "Done 2000\n",
      "Done 3000\n",
      "Done 4000\n",
      "Done 5000\n",
      "Done 6000\n"
     ]
    }
   ],
   "source": [
    "deeplift_model =\\\n",
    "    kc.convert_model_from_saved_files(\n",
    "        \"NN.h5\",\n",
    "        nonlinear_mxts_mode=deeplift.layers.NonlinearMxtsMode.DeepLIFT_GenomicsDefault) \n",
    "    \n",
    "find_scores_layer_idx = 0\n",
    "\n",
    "deeplift_contribs_func = deeplift_model.get_target_contribs_func(\n",
    "                            find_scores_layer_idx=find_scores_layer_idx,\n",
    "                            target_layer_idx=-2)\n",
    "\n",
    "X_train_deeplift = list()\n",
    "X_test_deeplift = list()\n",
    "\n",
    "for i in range(model.get_weights()[-1].shape[0]):\n",
    "    \n",
    "    train = np.array(deeplift_contribs_func(task_idx=i,\n",
    "                                             input_data_list=[X_train],\n",
    "                                             batch_size=10,\n",
    "                                             progress_update=1000))\n",
    "\n",
    "    test = np.array(deeplift_contribs_func(task_idx=i,\n",
    "                                             input_data_list=[X_test],\n",
    "                                             batch_size=10,\n",
    "                                             progress_update=1000))\n",
    "    \n",
    "    X_train_deeplift.append(train)\n",
    "    X_test_deeplift.append(test)\n",
    "    \n",
    "X_train_dl = np.array(X_train_deeplift)\n",
    "X_test_dl = np.array(X_test_deeplift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_deeplift = list()\n",
    "X_test_deeplift = list()\n",
    "\n",
    "for i in range(len(nn_preds_train)):\n",
    "    index = nn_preds_train[i]\n",
    "    X_train_deeplift.append(X_train_dl[index][i])\n",
    "\n",
    "for i in range(len(nn_preds_test)):\n",
    "    index = nn_preds_test[i]\n",
    "    X_test_deeplift.append(X_test_dl[index][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_deeplift = np.array(X_train_deeplift)\n",
    "X_test_deeplift = np.array(X_test_deeplift)\n",
    "\n",
    "np.save(\"X_train_deeplift\", X_train_deeplift)\n",
    "np.save(\"X_test_deeplift\", X_test_deeplift)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
