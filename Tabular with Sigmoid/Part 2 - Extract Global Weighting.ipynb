{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Extracting Global Feature Weighting:\n",
    "\n",
    "### Pedagogical \n",
    "1. Perturbation Method \n",
    "2. Sensitivity (Im et al. 2007) \n",
    "\n",
    "### Decompositional\n",
    "1. Garson's Algorithm. \n",
    "2. Connection Weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models and Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = pickle.load(open(\"k-nn_model.sav\", 'rb'))\n",
    "model = load_model(\"NN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"X_train.npy\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_test = np.load(\"y_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep track of what the numpy matrix columns are\n",
    "feature_names = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pedagogical\n",
    "### Sensitivity (Im et al. 2007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "S_i = \\frac{(\\sum_{L} \\frac{|P^0 - P^i|}{P^0})}{n}\n",
    "$$\n",
    "\n",
    "Where $P^0$ is the normal prediction value for each training instance after training. <br>\n",
    "$P^i$ is the modified prediction when input node is removed. <br>\n",
    "$L$ is the set of training data. <br>\n",
    "$n$ is the number of training data instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LIMIT_BAL': 0.36555495648113234,\n",
       " 'AGE': 0.09980291325518738,\n",
       " 'PAY_0': 4.088079244620865,\n",
       " 'PAY_2': 0.5472792245711559,\n",
       " 'PAY_3': 0.3929872988997995,\n",
       " 'PAY_4': 0.2914931618846628,\n",
       " 'PAY_5': 0.23134937200139513,\n",
       " 'PAY_6': 0.21140192810107375,\n",
       " 'BILL_AMT1': 0.19738257473548584,\n",
       " 'BILL_AMT2': 0.21833864640359632,\n",
       " 'BILL_AMT3': 0.08671430777171084,\n",
       " 'BILL_AMT4': 0.13725055436902614,\n",
       " 'BILL_AMT5': 0.12588156242955847,\n",
       " 'BILL_AMT6': 0.11677851517353671,\n",
       " 'PAY_AMT1': 0.14455251931410687,\n",
       " 'PAY_AMT2': 0.14977028905386153,\n",
       " 'PAY_AMT3': 0.14997392137235874,\n",
       " 'PAY_AMT4': 0.11766988622622163,\n",
       " 'PAY_AMT5': 0.09332286575252602,\n",
       " 'PAY_AMT6': 0.18032908941836406,\n",
       " 'SEX_1': 0.14792031993609636,\n",
       " 'SEX_2': 0.1339676630050999,\n",
       " 'EDUCATION_0': 0.2813087149665868,\n",
       " 'EDUCATION_1': 0.30166202735915915,\n",
       " 'EDUCATION_2': 0.2799833689026212,\n",
       " 'EDUCATION_3': 0.18653849067494108,\n",
       " 'EDUCATION_4': 0.21685637820624834,\n",
       " 'EDUCATION_5': 0.13520752381981704,\n",
       " 'EDUCATION_6': 0.2569881056167396,\n",
       " 'MARRIAGE_0': 0.13311750711265266,\n",
       " 'MARRIAGE_1': 0.21680821942995923,\n",
       " 'MARRIAGE_2': 0.15312789891392337,\n",
       " 'MARRIAGE_3': 0.22453134613027245}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate though each feature, remove it from the input data and evalutate above forumla for get weights\n",
    "\n",
    "sensitivity_weights = dict()\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    feature = feature_names[i]\n",
    "    temp_df = deepcopy(X_test)\n",
    "    \n",
    "    # Remove Input\n",
    "    temp_df = temp_df.T\n",
    "    temp_df[i] = 0\n",
    "    temp_df = temp_df.T\n",
    "    \n",
    "    numerator = 0\n",
    "    denomonator = len(X_test)\n",
    "    \n",
    "    for j in range(len(X_test)):\n",
    "        p0 = model.predict_proba(np.array([X_test[j]]))[0][0]\n",
    "        pi = model.predict_proba(np.array([temp_df[j]]))[0][0]\n",
    "        numerator += (abs(p0 - pi) / p0)\n",
    "        \n",
    "    si = numerator / denomonator\n",
    "    sensitivity_weights[feature] = si\n",
    "    \n",
    "sensitivity_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('sensitivity_weights.npy', sensitivity_weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbation Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ x = x +- \\sigma $\n",
    "\n",
    "Where $\\sigma$ is a perturbation of 0-20% of the input $x$. There was a recent paper by Runbo et al. http://www.jcomputers.us/vol6/jcp0607-16.pdf showing the optimal perturbation range to be 20%.\n",
    "\n",
    "Theodor et al. https://www.dcl.hpi.uni-potsdam.de/papers/papers/heinze-feature-salience.pdf also showed perturbation to be the most promising method of retrieving feature ranking from a NN when compared to their own algorithm and connection weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our Proposed Weighting Formulation\n",
    "$ W_i = \\frac{\\sum_{j=1}^{n}\\delta (\\sigma, L_j)}{2n} $ \n",
    "\n",
    "where $W_i$ represents the global weight of feature $i$, $\\sigma$ the perturbation range, $n$ the number of training set instances, $L$ the training data, and $\\delta$ a function returning the absolute summed change in two predictions with a positive and negative perturbation respectively to the feature $i$ in instance $L_j$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First +20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LIMIT_BAL': 291.8165729548782,\n",
       " 'AGE': 89.83324689976871,\n",
       " 'PAY_0': 2070.7176672723144,\n",
       " 'PAY_2': 407.48857374303043,\n",
       " 'PAY_3': 259.95386372320354,\n",
       " 'PAY_4': 221.7571383677423,\n",
       " 'PAY_5': 169.57525571249425,\n",
       " 'PAY_6': 165.49539516493678,\n",
       " 'BILL_AMT1': 170.75871409662068,\n",
       " 'BILL_AMT2': 135.77834891341627,\n",
       " 'BILL_AMT3': 64.19566868431866,\n",
       " 'BILL_AMT4': 108.24045379646122,\n",
       " 'BILL_AMT5': 87.28869246691465,\n",
       " 'BILL_AMT6': 114.4857925157994,\n",
       " 'PAY_AMT1': 64.2204763237387,\n",
       " 'PAY_AMT2': 88.2576067801565,\n",
       " 'PAY_AMT3': 105.54655730910599,\n",
       " 'PAY_AMT4': 57.70376958139241,\n",
       " 'PAY_AMT5': 59.57812053710222,\n",
       " 'PAY_AMT6': 136.43784589506686,\n",
       " 'SEX_1': 65.49494473077357,\n",
       " 'SEX_2': 52.31237966194749,\n",
       " 'EDUCATION_0': 114.07024168223143,\n",
       " 'EDUCATION_1': 138.53161038830876,\n",
       " 'EDUCATION_2': 141.79146474599838,\n",
       " 'EDUCATION_3': 87.04637013934553,\n",
       " 'EDUCATION_4': 119.96509080007672,\n",
       " 'EDUCATION_5': 61.282263837754726,\n",
       " 'EDUCATION_6': 97.12521517835557,\n",
       " 'MARRIAGE_0': 73.44656120613217,\n",
       " 'MARRIAGE_1': 113.17662553116679,\n",
       " 'MARRIAGE_2': 85.79414839111269,\n",
       " 'MARRIAGE_3': 87.57410073839128}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturb_up = dict()\n",
    "perturbation_range = 0.4  # +20% Since data is normalised between -1 => +1\n",
    "\n",
    "# Prepare dicitonary for feature weights\n",
    "for feature in feature_names:\n",
    "    perturb_up[feature] = 0\n",
    "\n",
    "# Get original prediction before perturbation\n",
    "for i in range(len(X_test)):\n",
    "    \n",
    "    model_output = model.predict_proba(np.array([X_test[i]]))[0][0]\n",
    "    \n",
    "    for j in range(len(feature_names)):\n",
    "        perturb_instance = deepcopy(X_test[i])\n",
    "        perturb_instance[j] += perturbation_range\n",
    "        \n",
    "        perturb_output = model.predict_proba(np.array([perturb_instance]))[0][0]\n",
    "        change = abs(model_output - perturb_output)\n",
    "        perturb_up[feature_names[j]] += change\n",
    "        \n",
    "perturb_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now -20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LIMIT_BAL': 357.02522126957774,\n",
       " 'AGE': 70.44857372716069,\n",
       " 'PAY_0': 438.43412312306464,\n",
       " 'PAY_2': 223.5622294358909,\n",
       " 'PAY_3': 160.6538814753294,\n",
       " 'PAY_4': 126.96168173104525,\n",
       " 'PAY_5': 121.96668748185039,\n",
       " 'PAY_6': 138.7631885614246,\n",
       " 'BILL_AMT1': 153.6645695734769,\n",
       " 'BILL_AMT2': 150.03131825849414,\n",
       " 'BILL_AMT3': 66.71190563030541,\n",
       " 'BILL_AMT4': 148.78792072087526,\n",
       " 'BILL_AMT5': 100.61124344542623,\n",
       " 'BILL_AMT6': 144.93596340715885,\n",
       " 'PAY_AMT1': 58.53202279098332,\n",
       " 'PAY_AMT2': 104.12175029329956,\n",
       " 'PAY_AMT3': 118.68184467405081,\n",
       " 'PAY_AMT4': 64.60520307347178,\n",
       " 'PAY_AMT5': 77.90462560392916,\n",
       " 'PAY_AMT6': 176.14139191433787,\n",
       " 'SEX_1': 61.64236407727003,\n",
       " 'SEX_2': 53.556175826117396,\n",
       " 'EDUCATION_0': 98.38287956826389,\n",
       " 'EDUCATION_1': 112.0876208646223,\n",
       " 'EDUCATION_2': 119.23504830244929,\n",
       " 'EDUCATION_3': 88.36225634627044,\n",
       " 'EDUCATION_4': 136.02513209730387,\n",
       " 'EDUCATION_5': 56.821483893319964,\n",
       " 'EDUCATION_6': 74.72056831791997,\n",
       " 'MARRIAGE_0': 72.25396698713303,\n",
       " 'MARRIAGE_1': 107.74383977428079,\n",
       " 'MARRIAGE_2': 82.78497367538512,\n",
       " 'MARRIAGE_3': 76.89556426927447}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturb_down = dict()\n",
    "perturbation_range = -0.4  # +20% Since data is normalised between -1 => +1\n",
    "\n",
    "# Prepare dicitonary for feature weights\n",
    "for feature in feature_names:\n",
    "    perturb_down[feature] = 0\n",
    "\n",
    "# Get original prediction before perturbation\n",
    "for i in range(len(X_test)):\n",
    "    \n",
    "    model_output = model.predict_proba(np.array([X_test[i]]))[0][0]\n",
    "    \n",
    "    for j in range(len(feature_names)):\n",
    "        perturb_instance = deepcopy(X_test[i])\n",
    "        perturb_instance[j] += perturbation_range\n",
    "        \n",
    "        perturb_output = model.predict_proba(np.array([perturb_instance]))[0][0]\n",
    "        change = abs(model_output - perturb_output)\n",
    "        perturb_down[feature_names[j]] += change\n",
    "        \n",
    "perturb_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LIMIT_BAL': 0.013517537379676165,\n",
       " 'AGE': 0.0033392045963943624,\n",
       " 'PAY_0': 0.052273995633237064,\n",
       " 'PAY_2': 0.013146891732894194,\n",
       " 'PAY_3': 0.008762661358302769,\n",
       " 'PAY_4': 0.0072649754187247405,\n",
       " 'PAY_5': 0.0060737904832155135,\n",
       " 'PAY_6': 0.006338720494299196,\n",
       " 'BILL_AMT1': 0.0067588184097937,\n",
       " 'BILL_AMT2': 0.0059543680660814665,\n",
       " 'BILL_AMT3': 0.0027272411315546682,\n",
       " 'BILL_AMT4': 0.00535475780244451,\n",
       " 'BILL_AMT5': 0.003914581998173768,\n",
       " 'BILL_AMT6': 0.00540461991506163,\n",
       " 'PAY_AMT1': 0.0025573437315567086,\n",
       " 'PAY_AMT2': 0.004007903272363667,\n",
       " 'PAY_AMT3': 0.004671425041315767,\n",
       " 'PAY_AMT4': 0.002548103596976337,\n",
       " 'PAY_AMT5': 0.002864223877938154,\n",
       " 'PAY_AMT6': 0.006512067454362599,\n",
       " 'SEX_1': 0.0026486939335009082,\n",
       " 'SEX_2': 0.0022055949060013516,\n",
       " 'EDUCATION_0': 0.004426106692718652,\n",
       " 'EDUCATION_1': 0.0052212339844360635,\n",
       " 'EDUCATION_2': 0.005438052355175993,\n",
       " 'EDUCATION_3': 0.003654346385116999,\n",
       " 'EDUCATION_4': 0.005333129643695429,\n",
       " 'EDUCATION_5': 0.0024604947443973894,\n",
       " 'EDUCATION_6': 0.0035801204895057406,\n",
       " 'MARRIAGE_0': 0.003035427670693025,\n",
       " 'MARRIAGE_1': 0.004602509693863491,\n",
       " 'MARRIAGE_2': 0.0035120650430520376,\n",
       " 'MARRIAGE_3': 0.00342645135432637}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average Scores going up and down\n",
    "perturb_weights = dict()\n",
    "\n",
    "for key, value in perturb_up.items():\n",
    "    # Divide the values by the number of training samples to normalise the results\n",
    "    # Also divide by two to normalise the up and down sampling\n",
    "    perturb_weights[key] = ( perturb_up[key] + perturb_down[key] ) / ( 2 * len(X_train) )\n",
    "    \n",
    "perturb_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('perturb_weights.npy', perturb_weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompositional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Garson's Algorithm\n",
    "Shown by Olden et al. 2002 ftp://gis.msl.mt.gov/Maxell/Models/Predictive_Modeling_for_DSS_Lincoln_NE_121510/Modeling_Literature/Olden_ANN's.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Calculate the product of each input weight with the hidden layer weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "second_layer_weights = model.layers[2].get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hidden_nodes, inputs)\n",
    "W1 = first_layer_weights.T\n",
    "W2 = second_layer_weights.T\n",
    "Qih_dict = dict()\n",
    "\n",
    "for h in range(len(W1)):\n",
    "    for i in range(len(W1[0])):\n",
    "        Qih_dict[\"H_\" + str(h) + \"I_\" + str(i)] = W1[h][i] * W2[0][h]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Relative Contribution\n",
    "For each input neuron to the outgoing signal of each hidden neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qih_dict2 = dict()\n",
    "\n",
    "for h in range(len(W1)):\n",
    "    for i in range(len(W1[0])):\n",
    "        \n",
    "        numerator = abs(Qih_dict[\"H_\" + str(h) + \"I_\" + str(i)])\n",
    "        denomonator = 0\n",
    "        \n",
    "        for j in range(len(W1[0])):\n",
    "            denomonator += abs(Qih_dict[\"H_\" + str(h) + \"I_\" + str(j)])\n",
    "            \n",
    "        Qih_dict2[\"H_\" + str(h) + \"I_\" + str(i)] = numerator / denomonator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also sum input neuron contributions\n",
    "for i in range(len(W1[0])):\n",
    "    total = 0\n",
    "    for h in range(len(W1)):\n",
    "        total += Qih_dict2[\"H_\" + str(h) + \"I_\" + str(i)]\n",
    "    Qih_dict2[\"Sum_input_\" + str(i)] = total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Relative Contribution of each input variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "garsons_weights = dict()\n",
    "\n",
    "for i in range(len(W1[0])):\n",
    "    total = 0\n",
    "    for j in range(len(W1[0])):\n",
    "        total += Qih_dict2[\"Sum_input_\" + str(j)]\n",
    "    garsons_weights[\"Input_\" + str(i)] = Qih_dict2[\"Sum_input_\" + str(i)] / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(feature_names)):\n",
    "    feature = feature_names[i]\n",
    "    garsons_weights[feature] = garsons_weights.pop(\"Input_\" + str(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LIMIT_BAL': 0.04395260965971895,\n",
       " 'AGE': 0.026944057167293797,\n",
       " 'PAY_0': 0.08343179461817793,\n",
       " 'PAY_2': 0.042876711847940556,\n",
       " 'PAY_3': 0.0359065599004902,\n",
       " 'PAY_4': 0.03532229472444586,\n",
       " 'PAY_5': 0.04251643962484055,\n",
       " 'PAY_6': 0.042533212769370256,\n",
       " 'BILL_AMT1': 0.029527755927775734,\n",
       " 'BILL_AMT2': 0.031253182778824405,\n",
       " 'BILL_AMT3': 0.02446191531544625,\n",
       " 'BILL_AMT4': 0.032938582451137906,\n",
       " 'BILL_AMT5': 0.02246390869742399,\n",
       " 'BILL_AMT6': 0.03718398346177904,\n",
       " 'PAY_AMT1': 0.022172380196296755,\n",
       " 'PAY_AMT2': 0.0269092586235034,\n",
       " 'PAY_AMT3': 0.024459912479915906,\n",
       " 'PAY_AMT4': 0.029573208742359886,\n",
       " 'PAY_AMT5': 0.018185671530820045,\n",
       " 'PAY_AMT6': 0.019636264385791632,\n",
       " 'SEX_1': 0.021906556147777038,\n",
       " 'SEX_2': 0.023203353136078075,\n",
       " 'EDUCATION_0': 0.024656752702645154,\n",
       " 'EDUCATION_1': 0.030158694252737376,\n",
       " 'EDUCATION_2': 0.025891523554172294,\n",
       " 'EDUCATION_3': 0.027243084603266812,\n",
       " 'EDUCATION_4': 0.02914003814046445,\n",
       " 'EDUCATION_5': 0.02696126882212165,\n",
       " 'EDUCATION_6': 0.023007187624478474,\n",
       " 'MARRIAGE_0': 0.025740288051310057,\n",
       " 'MARRIAGE_1': 0.02345829211714184,\n",
       " 'MARRIAGE_2': 0.024367664304447267,\n",
       " 'MARRIAGE_3': 0.022015591640006885}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "garsons_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('garsons_weights.npy', garsons_weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection Weights Algorithm\n",
    "Introducted by Olden et al. ftp://gis.msl.mt.gov/Maxell/Models/Predictive_Modeling_for_DSS_Lincoln_NE_121510/Modeling_Literature/Olden_ANN's.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ R_{ij} = \\sum_{k=1}^{H}W_{ik}.W_{kj} $\n",
    "\n",
    "Where $k$ is a hidden neuron, $i$ is the input neuron and $j$ is the output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_weights = dict()\n",
    "W1 = first_layer_weights\n",
    "W2 = second_layer_weights\n",
    "\n",
    "for i in range(len(W1)):\n",
    "    total = 0\n",
    "    for h in range(len(W1[0])):\n",
    "        total += W1[i][h] * W2[h]\n",
    "    connection_weights[\"Input_\" + str(i)] = total[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(feature_names)):\n",
    "    feature = feature_names[i]\n",
    "    connection_weights[feature] = connection_weights.pop(\"Input_\" + str(i)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LIMIT_BAL': -1.300265,\n",
       " 'AGE': 0.73999935,\n",
       " 'PAY_0': 3.2776766,\n",
       " 'PAY_2': -3.7349591,\n",
       " 'PAY_3': -0.12929907,\n",
       " 'PAY_4': -0.43788692,\n",
       " 'PAY_5': 0.70825404,\n",
       " 'PAY_6': 0.4893832,\n",
       " 'BILL_AMT1': 1.1666768,\n",
       " 'BILL_AMT2': 0.39782614,\n",
       " 'BILL_AMT3': 0.02625596,\n",
       " 'BILL_AMT4': -0.3857166,\n",
       " 'BILL_AMT5': 0.6940872,\n",
       " 'BILL_AMT6': -0.9198411,\n",
       " 'PAY_AMT1': -0.023594515,\n",
       " 'PAY_AMT2': 0.14318544,\n",
       " 'PAY_AMT3': -1.1518391,\n",
       " 'PAY_AMT4': 0.44849443,\n",
       " 'PAY_AMT5': -0.009882068,\n",
       " 'PAY_AMT6': -0.8673515,\n",
       " 'SEX_1': 0.105939336,\n",
       " 'SEX_2': -0.010222288,\n",
       " 'EDUCATION_0': 0.3701133,\n",
       " 'EDUCATION_1': 0.7569208,\n",
       " 'EDUCATION_2': 0.88181597,\n",
       " 'EDUCATION_3': 0.55383164,\n",
       " 'EDUCATION_4': -0.5475441,\n",
       " 'EDUCATION_5': 0.5473438,\n",
       " 'EDUCATION_6': 0.7352612,\n",
       " 'MARRIAGE_0': -0.7086076,\n",
       " 'MARRIAGE_1': 1.097823,\n",
       " 'MARRIAGE_2': 0.85597163,\n",
       " 'MARRIAGE_3': 0.78602546}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('connection_weights.npy', connection_weights) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
