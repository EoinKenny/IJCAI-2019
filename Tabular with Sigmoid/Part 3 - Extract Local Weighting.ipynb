{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3: Extract Local Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "\n",
    "# For Deep Learning Explanations\n",
    "from deepexplain.tensorflow import DeepExplain\n",
    "import deeplift\n",
    "from deeplift.conversion import kerasapi_conversion as kc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"X_train.npy\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "y_test = np.load(\"y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Generate Gradient Matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"NN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_predict(i):\n",
    "    \"\"\"\n",
    "    LIME doesn't support the format Keras uses, so we need a small helper function\n",
    "    \n",
    "    This just predicts a probability and gives the alternative also.\n",
    "    \"\"\"\n",
    "    \n",
    "    global model\n",
    "    \n",
    "    probability_yes = model.predict_proba(i)\n",
    "    x = np.zeros((probability_yes.shape[0], 1))\n",
    "    probability_no = (x + 1) - probability_yes\n",
    "    final = np.append(probability_no, probability_yes, axis=1)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB\n",
    "Although the use of LIME here is incorrect regarding the integration of categorical variables, it is necessary for experiments because we need coefficients for all features, including all one hot encoded features. Nugent and Cunningham's paper requires this approach to implement it. \n",
    "\n",
    "If using LIME to acquire standard explanations, please refer to https://github.com/marcotcr/lime for the proper treatment of categorical variables. It was discovered that this approach worked better for $agreement$ scores in experiments than implementing our own code to implement Nugent and Cunningham's method from scratch (as they didn't provide code with their publication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_features_ohe(df):\n",
    "    \"\"\"\n",
    "    Locate which features are categorical ohe, they should just have 2 values 0 and 1\n",
    "    \n",
    "    return: a list of integers referring to the ohe indexes\n",
    "    \"\"\"\n",
    "    \n",
    "    categorical_features = list()\n",
    "    for i in range(len(df.columns)):\n",
    "        if df[df.columns[i]].value_counts().shape == (2,):\n",
    "            categorical_features.append(i)\n",
    "    return categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = which_features_ohe(df)\n",
    "feature_names = df.columns\n",
    "num_features = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, \n",
    "                                                   feature_names=feature_names,\n",
    "                                                   categorical_features=categorical_features,\n",
    "                                                   class_names=['N', \"Y\"],\n",
    "                                                   verbose=False, \n",
    "                                                   discretize_continuous=False,\n",
    "                                                   mode='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % done...\n",
      "4.166666666666666 % done...\n",
      "8.333333333333332 % done...\n",
      "12.5 % done...\n",
      "16.666666666666664 % done...\n",
      "20.833333333333336 % done...\n",
      "25.0 % done...\n",
      "29.166666666666668 % done...\n",
      "33.33333333333333 % done...\n",
      "37.5 % done...\n",
      "41.66666666666667 % done...\n",
      "45.83333333333333 % done...\n",
      "50.0 % done...\n",
      "54.166666666666664 % done...\n",
      "58.333333333333336 % done...\n",
      "62.5 % done...\n",
      "66.66666666666666 % done...\n",
      "70.83333333333334 % done...\n",
      "75.0 % done...\n",
      "79.16666666666666 % done...\n",
      "83.33333333333334 % done...\n",
      "87.5 % done...\n",
      "91.66666666666666 % done...\n",
      "95.83333333333334 % done...\n"
     ]
    }
   ],
   "source": [
    "X_train_grad_LIME = list()\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print((i / len(X_train)) * 100, \"% done...\")\n",
    "    \n",
    "    qc = X_train[i]\n",
    "    exp = explainer.explain_instance(qc, flatten_predict, num_features=num_features)\n",
    "    \n",
    "    # Get real coefficients\n",
    "    coefs = exp.as_map()[1]\n",
    "    coefs.sort()\n",
    "    coefs = [x[1] for x in coefs]\n",
    "\n",
    "    X_train_grad_LIME.append(np.append(coefs, exp.intercept[1]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % done...\n",
      "16.666666666666664 % done...\n",
      "33.33333333333333 % done...\n",
      "50.0 % done...\n",
      "66.66666666666666 % done...\n",
      "83.33333333333334 % done...\n"
     ]
    }
   ],
   "source": [
    "X_test_grad_LIME = list()\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print((i / len(X_test)) * 100, \"% done...\")\n",
    "    \n",
    "    qc = X_test[i]\n",
    "    exp = explainer.explain_instance(qc, flatten_predict, num_features=num_features)\n",
    "    \n",
    "    # Get real coefficients\n",
    "    coefs = exp.as_map()[1]\n",
    "    coefs.sort()\n",
    "    coefs = [x[1] for x in coefs]\n",
    "\n",
    "    X_test_grad_LIME.append(np.append(coefs, exp.intercept[1]).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_grad_LIME = np.array(X_train_grad_LIME)\n",
    "X_test_grad_LIME = np.array(X_test_grad_LIME)\n",
    "np.save(\"X_train_grad_LIME\", X_train_grad_LIME)\n",
    "np.save(\"X_test_grad_LIME\", X_test_grad_LIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrated Gradients and Layerwise Relevance Propagation\n",
    "https://github.com/marcoancona/DeepExplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepExplain: running \"intgrad\" explanation method (3)\n",
      "Model with multiple inputs:  False\n",
      "DeepExplain: running \"elrp\" explanation method (4)\n",
      "Model with multiple inputs:  False\n",
      "DeepExplain: running \"intgrad\" explanation method (3)\n",
      "Model with multiple inputs:  False\n",
      "DeepExplain: running \"elrp\" explanation method (4)\n",
      "Model with multiple inputs:  False\n"
     ]
    }
   ],
   "source": [
    "with DeepExplain(session=K.get_session()) as de:  # <-- init DeepExplain context\n",
    "    input_tensor = model.layers[0].input\n",
    "    fModel = Model(inputs=input_tensor, outputs = model.layers[-2].output)\n",
    "    target_tensor = fModel(input_tensor)\n",
    "    \n",
    "    xs = X_train\n",
    "    \n",
    "    X_train_intgrad = de.explain('intgrad', target_tensor, input_tensor, xs)\n",
    "    X_train_lrp = de.explain('elrp', target_tensor, input_tensor, xs)\n",
    "    \n",
    "    \n",
    "with DeepExplain(session=K.get_session()) as de:  # <-- init DeepExplain context\n",
    "    input_tensor = model.layers[0].input\n",
    "    fModel = Model(inputs=input_tensor, outputs = model.layers[-2].output)\n",
    "    target_tensor = fModel(input_tensor)\n",
    "    \n",
    "    xs = X_test\n",
    "    \n",
    "    X_test_intgrad = de.explain('intgrad', target_tensor, input_tensor, xs)\n",
    "    X_test_lrp = de.explain('elrp', target_tensor, input_tensor, xs)\n",
    "    \n",
    "np.save(\"X_train_intgrad\", X_train_intgrad)\n",
    "np.save(\"X_test_intgrad\", X_test_intgrad)\n",
    "np.save(\"X_train_lrp\", X_train_lrp)\n",
    "np.save(\"X_test_lrp\", X_test_lrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLIFT Contributions:\n",
    "https://github.com/kundajelab/deeplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nonlinear_mxts_mode is set to: DeepLIFT_GenomicsDefault\n",
      "For layer 1 the preceding linear layer is 0 of type Dense;\n",
      "In accordance with nonlinear_mxts_modeDeepLIFT_GenomicsDefault we are setting the NonlinearMxtsMode to RevealCancel\n",
      "Heads-up: I assume sigmoid is the output layer, not an intermediate one; if it's an intermediate layer then please bug me and I will implement the grad func\n",
      "For layer 3 the preceding linear layer is 2 of type Dense;\n",
      "In accordance with nonlinear_mxts_modeDeepLIFT_GenomicsDefault we are setting the NonlinearMxtsMode to RevealCancel\n",
      "No reference provided - using zeros\n",
      "Done 0\n",
      "Done 1000\n",
      "Done 2000\n",
      "Done 3000\n",
      "Done 4000\n",
      "Done 5000\n",
      "Done 6000\n",
      "Done 7000\n",
      "Done 8000\n",
      "Done 9000\n",
      "Done 10000\n",
      "Done 11000\n",
      "Done 12000\n",
      "Done 13000\n",
      "Done 14000\n",
      "Done 15000\n",
      "Done 16000\n",
      "Done 17000\n",
      "Done 18000\n",
      "Done 19000\n",
      "Done 20000\n",
      "Done 21000\n",
      "Done 22000\n",
      "Done 23000\n",
      "No reference provided - using zeros\n",
      "Done 0\n",
      "Done 1000\n",
      "Done 2000\n",
      "Done 3000\n",
      "Done 4000\n",
      "Done 5000\n"
     ]
    }
   ],
   "source": [
    "deeplift_model =\\\n",
    "    kc.convert_model_from_saved_files(\n",
    "        \"NN.h5\",\n",
    "        nonlinear_mxts_mode=deeplift.layers.NonlinearMxtsMode.DeepLIFT_GenomicsDefault) \n",
    "    \n",
    "find_scores_layer_idx = 0\n",
    "\n",
    "deeplift_contribs_func = deeplift_model.get_target_contribs_func(\n",
    "                            find_scores_layer_idx=find_scores_layer_idx,\n",
    "                            target_layer_idx=-2)\n",
    "\n",
    "X_train_deeplift = np.array(deeplift_contribs_func(task_idx=0,\n",
    "                                         input_data_list=[X_train],\n",
    "                                         batch_size=10,\n",
    "                                         progress_update=1000))\n",
    "\n",
    "X_test_deeplift = np.array(deeplift_contribs_func(task_idx=0,\n",
    "                                         input_data_list=[X_test],\n",
    "                                         batch_size=10,\n",
    "                                         progress_update=1000))\n",
    "\n",
    "\n",
    "np.save(\"X_train_deeplift\", X_train_deeplift)\n",
    "np.save(\"X_test_deeplift\", X_test_deeplift)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
